{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283732ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031f37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train is the features training set, with each interior array representing an instance\n",
    "# y_train is the array of targets from the training set\n",
    "X_train = np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "])\n",
    "y_train = np.array([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6450ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using gradient descent to try finding the optimal parameters in my logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab57430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll first need to make some guesses for my parameters\n",
    "m,n = X_train.shape # m is the size of the training set, n is the number of features\n",
    "w_init = np.zeros(n) # creating an array of zeros, the size of the number of features\n",
    "b_init = 0\n",
    "lam = 100 # the weightage of the regularization\n",
    "alpha = 0.0001 # good old alpha\n",
    "ite = 10000 # the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31162d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_calc(X,y,w,b,lamd):\n",
    "    m = X.shape[0]\n",
    "    j = 0\n",
    "    for i in range(m):\n",
    "        j += (-y[i]*(np.log(1/(1+np.exp(-np.dot(w,X[i])-b)))) - \n",
    "              (1-y[i])*(np.log(1 - 1/(1+np.exp(-np.dot(w,X[i])-b)))))\n",
    "    return j/(2*m) + lamd/(2*m)*np.sum(w**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ba5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_func(X,y,w,b,a,lamd,ite):\n",
    "    for k in range(ite):\n",
    "        m,n = X.shape\n",
    "        dj_dw = np.zeros(n)\n",
    "        dj_db = 0\n",
    "        for i in range(m):\n",
    "            err = 1/(1 + np.exp(-np.dot(w,X[i]) - b)) - y[i]\n",
    "            dj_db += err\n",
    "            for j in range(n):\n",
    "                dj_dw[j] += err*X[i,j]\n",
    "        w = w - a*dj_dw/m - a*lamd*w/m\n",
    "        b -= a*dj_db/m\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac73891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimised w = [ 0.01378292  0.00895167  0.00412042 -0.00071083]\n",
      "optimised b = -0.17073832668929526\n"
     ]
    }
   ],
   "source": [
    "w_fin,b_fin = grad_func(X_train,y_train,w_init,b_init,alpha,lam,ite)\n",
    "print(f\"optimised w = {w_fin}\")\n",
    "print(f\"optimised b = {b_fin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweet! Soon I'll be demonstrating whether this is accurate or not using some visual matplotlib!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
