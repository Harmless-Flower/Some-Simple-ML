{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283732ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    I will be using gradient descent to try finding the optimal parameters of a binary sample set.\n",
    "    While this code can be more condensed, I would like to explain my steps to finally reach the desired parameters\n",
    "    \n",
    "    To understand all of this, some basic knowledge of numpy as well as supervised machine learning will be required\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "031f37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    X_train is the features training set, with each interior array representing an instance\n",
    "    y_train is the array of targets from the training set\n",
    "    the data is pretty random, and can be modified to whatever is desired\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_train = np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "])\n",
    "y_train = np.array([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab57430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    I'll first need to make some guesses for my parameters\n",
    "    I'll start by letting my parameters all be 0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "w_init = np.zeros(X_train.shape[1]) # creating an array of zeros, the size of the number of features\n",
    "b_init = 0\n",
    "lamb = 100 # the weightage of the regularization I will be doing\n",
    "alpha = 0.0001 # the learning rate parameter\n",
    "ite = 10000 # the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c977936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The sigmoid function that is typical of logistic regression\n",
    "\n",
    "    We're defining sigmoid now to make later equations a bit less crowded\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    g = np.zeros_like(z)\n",
    "    \n",
    "    g = 1/(1 - np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31162d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Although we don't need to calculate this, this can be used to plot graphs and demonstrate how the cost decreases\n",
    "    as more and more iterations of gradient descent are performed\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cost_calc(X,y,w,b,lamd):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        j += -y[i]*(np.log(sigmoid(np.dot(w,X[i]) + b))) - (1-y[i])*(np.log(1 - 1/(sigmoid(np.dot(w,X[i]) + b))))\n",
    "    \n",
    "    return j/(2*m) + lamd/(2*m)*np.sum(w**2) # this second term is the regularization term, (I am ignoring b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ba5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT COMPLETED YET ###\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    The gradient function, this will determine the gradient for each iteration\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def grad_func(X,y,w,b,a,lamd):\n",
    "    \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        err = sigmoid(np.dot(w,X[i]) + b) - y[i]\n",
    "        dj_db += err\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err*X[i,j] + lamd*w[j]\n",
    "    \n",
    "    \n",
    "    return dj_dw,dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ee443",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT COMPLETED YET ###\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    This function is the final piece, it will perform gradient descent over the given iterations\n",
    "    \n",
    "    It will also capture an array of cost values at different iterations to allow for this process to be visualised\n",
    "\n",
    "\"\"\"\n",
    "def run_through_func(X,y,w,b,alpha_,lambda_,iterations_):\n",
    "    m = X.shape[0]\n",
    "    for k in range(ite):\n",
    "        dj_dw, dj_db = grad_func(X,y,w,b,alpha_,lambda_)\n",
    "        w -= alpha/m*dj_dw\n",
    "        b -= alpha/m*dj_db\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac73891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimised w = [ 0.01378292  0.00895167  0.00412042 -0.00071083]\n",
      "optimised b = -0.17073832668929526\n"
     ]
    }
   ],
   "source": [
    "### NOT COMPLETED YET ###\n",
    "\n",
    "w_fin,b_fin = grad_func(X_train,y_train,w_init,b_init,alpha,lam,ite)\n",
    "print(f\"optimised w = {w_fin}\")\n",
    "print(f\"optimised b = {b_fin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT COMPLETED YET ###\n",
    "\n",
    "# Sweet! Soon I'll be demonstrating whether this is accurate or not using some visual matplotlib!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
